## Handling Duplicate Data

Duplicate data occurs when one or more rows contain **identical information**, leading to redundancy and potentially skewed analysis results. These rows should usually be identified and removed before performing aggregation or modeling.

---

### Detecting Duplicates

| Task | Command | Description |
|------|----------|-------------|
| Identify duplicate rows | `df.duplicated()` | Returns `True` for each row that is a duplicate of a previous one |
| Count total duplicates | `df.duplicated().sum()` | Returns the number of duplicate rows |
| Display all duplicate rows | `df[df.duplicated(keep=False)]` | Shows all rows that have duplicates (not just the later ones) |

 **Parameter Tip:**  
> `keep` can be `'first'`, `'last'`, or `False`  
> - `'first'`: Marks all but the first occurrence as duplicate  
> - `'last'`: Marks all but the last occurrence as duplicate  
> - `False`: Marks *all* duplicates as `True`

---

### Removing Duplicates

| Task | Command | Description |
|------|----------|-------------|
| Remove duplicate rows | `df.drop_duplicates()` | Removes duplicate rows, keeping the first occurrence |
| Remove all duplicates & reset index | `df.drop_duplicates().reset_index(drop=True)` | Cleans dataset and resets row numbers |
| Keep only unique rows | `df = df[~df.duplicated()]` | Filters DataFrame to retain non-duplicate rows |

---

### Best Practices

- Always check for duplicates **before** aggregating or joining data.  
- Use `subset=['col1', 'col2']` in `.duplicated()` or `.drop_duplicates()` to detect duplicates based on specific columns.  
- Verify results with `df.shape` before and after removing duplicates.

```python
# Example: Remove duplicates based on 'ID' and 'Name'
df = df.drop_duplicates(subset=['ID', 'Name'])

